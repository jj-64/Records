---
title: "EDA"
author: "Jinane Jouni"
date: "Dec 19, 2025"
output: html_document
---

# ----- 0. Setup ---------------------------------------------------
Let numbers display in decimals and not scientific notation
```{r option}
options(scipen = 999)
```

Set seed also
```{r set seed}
set.seed(12345)
```

Library Records is essential to run most of the functions available on github.

To install
```{r install Records}
## library Records from Github
#devtools::install_github("jj-64/Records")
```

Load library
```{r load Records}
#library(Records)
devtools::load_all(".")
```


Check for required packages for install and loading
```{r load packages, message=FALSE, warning=TRUE, include=FALSE}
required_packages <- c(
  "tidyverse", 
  "moments", # skewness, kurtosis
  "dplyr", "tibble", "stats", 
  "factoextra", "FactoMineR",  ##PCA and visualization
  "corrplot", # correlation visualization
  "psych",        # describe()
  "GGally",       # ggpairs
  "matrixStats"  # fast row/column stats
)
## install if missing
install_if_missing(required_packages)

## Load libraries
load_package(required_packages = required_packages)

## Set caret parallel if desired (commented out by default)
library(doParallel)
# cl <- makeCluster(detectCores() - 1); registerDoParallel(cl)
```

# ----- 1. Load data ---------------------------------------

Load feature matrix
```{r load feature matrix}
## For oneline users
#data("feature_matrix", package = "Records")

## Locally
data("feature_matrix")
```

Promise data
```{r see data}
str(feature_matrix)
```
Basic dimensions
```{r Basic dimensions original}
cat("Observations:", nrow(feature_matrix), "\n")
cat("Features:", ncol(feature_matrix), "\n")
```
# ----- 2. Missingness and degeneracy --------------------------

Check Missing values per feature
```{r}
na_summary <- tibble(
  feature = names(feature_matrix),
  n_na    = colSums(is.na(feature_matrix)),
  pct_na  = colMeans(is.na(feature_matrix))
) %>% dplyr::arrange(desc(pct_na))

if(sum (na_summary$n_na) >0 ) print(na_summary %>% dplyr::filter(n_na > 0)) else print("no NA")
```
Remove NAs if needed. Note that database is large so removing few NAs won't bias the analysis.
```{r remove NAs}
feature_matrix=feature_matrix[complete.cases(feature_matrix),]
```

new basic dimensions after NAs
```{r basic dimensions after NAs} 
cat("Observations:", nrow(feature_matrix), "\n")
cat("Features:", ncol(feature_matrix), "\n")
```

# ----- 3. Feature_only data ----------------------------------
Most of the analysis will be on the matrix containing only numeric values.
We will exclude other idetification columns such as label, series_id, label_m (submodel), series, and series length.
We obtain the "feature_only" matrix.
```{r feature only}
feature_names <- lubridate::setdiff(names(feature_matrix), c("Max_logLik","series_id", "label", "label_m", "series", "T_length"))

feature_only <- feature_matrix[, colnames(feature_matrix) %in% (feature_names)]
head(feature_only,1)
# feature_only <- feature_matrix[, sapply(feature_matrix, is.numeric)]
```

In case there are any problems with the feature_only matrix, we will convert all columns to numerical
```{r all numeric}
feature_only <- feature_only %>% mutate_if(is.factor, as.character) %>% mutate_all(as.numeric)
```

For later use, we would need to keep the label of each series stored in "regime" vector.
```{r regime}
regime = feature_matrix$label
```

Basic dimensions
```{r Basic dimensions feature_only}
cat("Observations:", nrow(feature_only), "\n")
cat("Features:", ncol(feature_only), "\n")
```
# ----- 4. Degeneracy ---------------------------------------
Check for constant or near-constant features.
```{r variance_summary}
variance_summary <- tibble(
  feature = names(feature_only),
  variance = apply(feature_only, 2, var, na.rm = TRUE),
  unique_vals = apply(feature_only, 2, function(z) length(unique(z)))
) %>% dplyr::arrange(variance)
```

These features will be removed immediately as they are obviously redundant and degenerate.

      Interpretation: 

      Zero variance â†’ remove
      
      Two unique values â†’ possible indicator, verify meaning
      
      High NA â†’ consider imputation or exclusion
```{r degenrate features}
degenerate_features <- subset(variance_summary , variance == 0 | unique_vals <=2)

if (nrow(degenerate_features) > 0) {
  #warning("Degenerate or quasi-degenerate features detected")
  print(degenerate_features)
}
```

Remove degenerate features
```{r remove degenrate features}
feature_only = feature_only[, !(colnames(feature_only) %in%  degenerate_features$feature)]
```

For later processing, replace -Inf with -1e5 ( only fix LogLik values)
```{r remove -Inf for logLik}
feature_only[grep("^logLik_", names(feature_only))] <-
  lapply(feature_only[grep("^logLik_", names(feature_only))], function(x) {
    x[x < -1e5] <- -1e5#Inf
    x[!is.finite(x)] <- -1e5
    x
  })
```

Save for now
```{r}
save(feature_only, file = "feature_only.rda")
```

# ----- 5 . Supervised PCA (Sanity Check) ----------------

The aim is to check if generated dataset is really separable and can be well identified. i.e if the four models are well identified.
```{r PCA}
pca <- FactoMineR::PCA(
  feature_only,
  scale.unit = TRUE,
  graph = FALSE
)
```

Variance explained Scree plot

Interpretation : Few PCs explaining most variance â†’ redundancy

We aim to have large number of PCs to explain more than 70-90% of the variance.

    Look for: 
    
    Sharp drop (elbow)
    
    Flat tail â†’ noise
```{r scree plot}
factoextra::fviz_eig(pca, addlabels = TRUE)
```

How many components should you keep?

You should not keep components based on visualization alone.

Explained variance criterion (baseline)
      
      Rules of thumb:

      1 - Keep components explaining 70â€“90% of variance

      2 - Typically yields 10â€“30 components from 100 variables
```{r PC to keep}
which(cumsum(pca$eig[,2]) >= 90)[1]
```
How to use PCA with known groups

We have 4 known models, so PCA should be combined with a supervised method.

Visual inspection only (exploratory)
Plot individuals and should be clearly clustered into four distinct groups.
```{r ind plot pca}
factoextra::fviz_pca_ind(pca,
             habillage = regime,
             addEllipses = TRUE,
             ellipse.level = 0.95
)
```


Feature Contribution to first PCs

We have features in all dimensions -> less redundancy
```{r var plot pca}
#loadings
# print(pca$var$coord)

factoextra::fviz_pca_var(pca, col.var = "contrib")

```

which variables define each PC
```{r var contrib pca}
head(pca$var$contrib, 10)
```

# ----- 6. k-means preview -----------------------
The elbow Method helps determine the optimal number of clusters.

There should be 4 clusters each for one model.

```{r elbow method kmeans}
factoextra::fviz_nbclust(scale(feature_only), kmeans, method = "wss")
```
Run k-means
```{r kmeans model}
k_means_result <- stats::kmeans(scale(feature_only), centers = 4, nstart = 20)
```

Plot kmeans. They should be 4 distinct clusters

```{r kmeans plot}
factoextra::fviz_cluster(list(data = feature_only, cluster = regime),
             ellipse.type = "convex",
             palette = "jco",
             main = "Actual Species Labels (Supervised Plot)")
```

Clusters

Check whether features contain structure before labels

If clusters align with known regimes â†’ strong signal

If not â†’ classification may rely on subtle margins
```{r clusters}
clusters = data.frame( features = regime, submodel = feature_matrix$label_m, cluster = k_means_result$cluster)
table(clusters)
```
Run k-means on variables reduction
```{r elbow method kmeans of variables}
factoextra::fviz_nbclust(scale(t(feature_only)), kmeans, method = "wss")
```

Plot kmeans. They should be a lot of distinct clusters
If a cluster has a lot of variables, they may be redundant (such as the logLik values)
```{r kmeans plot variables}
kmeans_result_var <- stats::kmeans(t(feature_only), centers = 6, nstart = 25)

# Visualize the k-means results
factoextra::fviz_cluster(kmeans_result_var, data = t(feature_only),
             ellipse.type = "convex",
             palette = "jco",
             main = "K-Means Cluster Visualization (factoextra)",
             labelsize = 6, # Set a small font size for observation labels
             outlier.labelsize = 6)
```

# ----- 7. Univariate Description ---------------------------
Univariate summary statistics
    
    What to look for
    
    Heavy skewness â†’ log / rank transforms
    
    Extreme kurtosis â†’ tail-driven statistics (records, extremes)

```{r describe stat}
desc_stats <- psych::describe(feature_only)

print(desc_stats)
```

# ----- 8.  Correlation analysis ----------------------------

Pairwise correlation (Spearman recommended)
```{r cor mat}
cor_mat <- cor(feature_only, use = "pairwise.complete.obs", method = "spearman")

```


Visual inspection

We aim to have only clear (low correlation) zones.
```{r corrplot}
corrplot::corrplot(cor_mat,
         method = "color",
         type = "upper",
         tl.cex = 0.3,
         tl.col = "black",
         order = "hclust", #c("original", "AOE", "FPC", "hclust", "alphabet"),
         rect.lwd = 3
         )
```

Identify highly correlated pairs

    Decision rule

    |Ï| > 0.95 â†’ keep one representative
    Prefer:
    More interpretable
    Lower variance inflation
    Better downstream SHAP signal

```{r high corr}
high_corr <- which(abs(cor_mat) > 0.95 & abs(cor_mat) < 1, arr.ind = TRUE) 

high_corr_pairs <- tibble(
  feature_1 = rownames(cor_mat)[high_corr[,1]],
  feature_2 = colnames(cor_mat)[high_corr[,2]],
  rho = cor_mat[high_corr]
   ) %>% dplyr::filter(feature_1 != feature_2)

high_corr_pairs <- high_corr_pairs %>%
  rowwise() %>%
  dplyr::mutate(pair = paste(sort(c(feature_1, feature_2)), collapse = "_")) %>%
  ungroup() %>%
  dplyr::distinct(pair, .keep_all = TRUE) %>%
  dplyr::select(-pair) %>% arrange(feature_1)


print(high_corr_pairs)
```


# ----- 9. Outputs ------------------------------------------

Save output to R data

```{r save output EDA}
eda_output <- list(
  desc_stats = desc_stats,
  na_summary = na_summary,
  variance_summary = variance_summary,
  correlation_matrix = cor_mat,
  pca = pca,
  kmeans = k_means_result
)

saveRDS(eda_output, "C:/Users/User/Documents/Records/inst/extdata/feature_EDA_summary.rds")
```

# ------ Group-wise summaries -----------------------

```{r group desc}
group_desc <- feature_only %>%
  dplyr::bind_cols(label = feature_matrix$label) %>%
  dplyr::group_by(label) %>%
  dplyr::summarise(
    across(
      where(is.numeric),
      list(
        mean = ~ mean(.x, na.rm = TRUE),
        #sd   = ~ sd(.x, na.rm = TRUE),
        med  = ~ median(.x, na.rm = TRUE)
        #IQR  = ~ IQR(.x, na.rm = TRUE)
      ),
      .names = "{.col}_{.fn}"
    ),
    .groups = "drop"
  ) %>%
  dplyr::select(-label) %>%
  t() %>%
  as.data.frame() %>%
  tibble::rownames_to_column("feature") %>%
  setNames(c("feature", "Classical", "DTRW", "LDM", "YNM")) %>%
  dplyr::mutate(across(-feature, as.numeric)) %>%
  rowwise() %>%
  dplyr::mutate(
    CV = abs(
      sd(c_across(Classical:YNM)) /
      mean(c_across(Classical:YNM))
    )
  ) %>%
    dplyr::mutate(
    range = abs(
      diff(range(c_across(Classical:YNM))) /
      mean(c_across(Classical:YNM))
    )
  ) %>%
  dplyr::mutate(
    MAD = abs(
      mad(c_across(Classical:YNM)) /
      median(abs(c_across(Classical:YNM)))
    )
  ) %>%
  ungroup() %>% 
  arrange(CV) %>%
  dplyr::mutate(across(where(is.numeric), \(x) round(x, digits = 4)))


print(head(group_desc, 10))
```

Variability is computed as the coefficient of variation.

    Interpretation
    
    CV < 0.05â€“0.10 â†’ essentially invariant

    CV 0.10â€“0.30 â†’ moderate differences

    CV > 0.30 â†’ strong model dependence

CV is unstable when the mean is close to zero. If some features have means near zero across models, CV can be misleadingly large. 
```{r group desc CV}
print(group_desc %>% dplyr::filter(CV < 0.1))
```

Before relying on CV, check also range which is less sensitive to distributional changes but more sensitive to outliers:
```{r group desc range}
print(group_desc %>% dplyr::filter(range < 0.3))
```

One can aslo check Normalized MAD (robust to outliers)
```{r group desc MAD}
print(group_desc %>% dplyr::filter(MAD < 0.3))
```

Intraclass Correlation Coefficient (ICC) â€“ Recommended

Since our goal is redundancy / agreement across models, ICC is statistically principled.

    ICC3 â‰ˆ 1 â†’ models give the same value â†’ redundant

    ICC3 â‰ˆ 0 â†’ no agreement â†’ informative difference

Conceptually:

â€œHow much of the variance is due to the feature itself rather than the model?â€

    Pros:
    
    Designed exactly for agreement assessment
    
    Interpretable and comparable
    
    Cons:
    
    Slightly more complex to compute
```{r}
psych::ICC(group_desc[,2:5], missing = TRUE)
```

# ----- Effect size screening (preferred over p-values) -----

Treat model as a factor and compute:
    
    ðœ‚2 = SSmodel /SStotal

    Î·Â² â‰ˆ 0 â†’ redundant across models
    Î·Â² < 0.01 â†’ negligible
    0.01â€“0.06 â†’ small
    0.06 â†’ meaningful
    Î·Â² large â†’ strong model effect

This answers:

â€œHow much variance is explained by the model choice?â€

```{r anova}
eta2_summary <- lapply(
  names(feature_only)[sapply(feature_only, is.numeric)],
  function(f) {
    fit <- aov(feature_only[[f]] ~ regime)
    data.frame(
      feature = f,
      eta2 = round(effectsize::eta_squared(fit, partial = FALSE)$Eta2,3)
    )
  }
) %>% bind_rows()

eta2_summary <- eta2_summary %>%
  arrange(desc(eta2))
```

recommended variables to remove below 0.02
```{r anova var to remove}
eta2_summary %>% filter(eta2 < 0.01)  %>% arrange(eta2)
```

# ----- Non-Parameteric Seperation: Kruskal -----------

Non Parameteric Kruskal test
```{r kruskal test}
kw_summary <- lapply(
  names(feature_only)[sapply(feature_only, is.numeric)],
  function(f) {
    kt <- kruskal.test(feature_only[[f]] ~ regime)
    data.frame(
      feature = f,
      p_value = round(kt$p.value,5)
    )
  }
) %>% bind_rows() %>%
  arrange(p_value)
```


```{r kruskal var to remove}
print(kw_summary[which(kw_summary$p_value < 0.01 | is.na(kw_summary$p_value)) , ])
```

# ---- Regime-conditioned plots ------------------------

Add the label to data and pivot longer
```{r pivot longer}
X_long <- cbind(feature_only,regime) %>%
  pivot_longer(
    cols = where(is.numeric),
    names_to = "feature",
    values_to = "value"
  )
```

Get unique features and split into groups of, say, 16
```{r feature chunks}
feature_list <- unique(X_long$feature)
feature_chunks <- split(feature_list, ceiling(seq_along(feature_list) / 8))
```

Density overlays
```{r density overlays}
plots <- map(feature_chunks, ~ {
  ggplot(filter(X_long, feature %in% .x), aes(x = value, fill = regime)) +
    geom_density(alpha = 0.3) +
    facet_wrap(~ feature, scales = "free", ncol = 2) +
    theme_minimal()
})
```

Boxplots by regime
```{r boxplot overlays}
plotsb <- map(feature_chunks, ~ {
  ggplot(filter(X_long, feature %in% .x), aes(x = regime, y = value)) +
    geom_boxplot(outlier.alpha = 0.2) +
    facet_wrap(~ feature, scales = "free", ncol = 2) +
    theme_minimal()
})
```

display plots
```{r display plots}
plots[[1]]
plotsb[[1]]
```

# ----- Feature removal ----------------------------

```{r feature removal}
remove_features = eta2_summary %>%
  filter(eta2 < 0.02)%>%
  pull(feature)

remove_features = c(remove_features, kw_summary %>%
  filter(p_value > 0.05) %>%
  pull(feature)
)

print(unique(remove_features))
```
Filtered feature set
```{r Filtered feature set}
selected_features <- setdiff(names(feature_matrix), unique(remove_features))

feature_matrix_reduced <- feature_matrix %>%
  select(all_of(selected_features))
```
