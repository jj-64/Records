---
title: "EDA"
author: "Jinane Jouni"
date: "`r Sys.Date()`"
output: html_document
---

# ----- 0. Setup ---------------------------------------------------
Let numbers display in decimals and not scientific notation
```{r option}
options(scipen = 999)
```

Set seed also
```{r set seed}
set.seed(12345)
```

Library Records is essential to run most of the functions available on github.

To install
```{r install Records}
## library Records from Github
#devtools::install_github("jj-64/Records")
```

Load library
```{r load Records}
#library(Records)
devtools::load_all(".")
```


Check for required packages for install and loading
```{r load packages, message=FALSE, warning=TRUE, include=FALSE}
required_packages <- c(
  "tidyverse", 
  "moments", # skewness, kurtosis
  "dplyr", "tibble", "stats", 
  "factoextra", "FactoMineR",  ##PCA and visualization
  "corrplot", # correlation visualization
  "psych",        # describe()
  "GGally",       # ggpairs
  "matrixStats"  # fast row/column stats
)
## install if missing
install_if_missing(required_packages)

## Load libraries
load_package(required_packages = required_packages)

## Set caret parallel if desired (commented out by default)
library(doParallel)
# cl <- makeCluster(detectCores() - 1); registerDoParallel(cl)
```

# ----- 1. Load data ---------------------------------------

Load feature matrix
```{r load feature matrix}
#data("feature_matrix", package = "Records")
load("C:/Users/User/Documents/Records/data/feature_matrix.rda")
```

Promise data
```{r see data}
str(feature_matrix)
```
Basic dimensions
```{r Basic dimensions}
n_obs  <- nrow(feature_matrix)
n_feat <- ncol(feature_matrix)

cat("Observations:", n_obs, "\n")
cat("Features:", n_feat, "\n")
```
# ----- 2. Missingness and degeneracy --------------------------

Check Missing values per feature
```{r}
na_summary <- tibble(
  feature = names(feature_matrix),
  n_na    = colSums(is.na(feature_matrix)),
  pct_na  = colMeans(is.na(feature_matrix))
) %>% dplyr::arrange(desc(pct_na))

if(sum (na_summary$n_na) >0 ) print(na_summary %>% dplyr::filter(n_na > 0)) else print("no NA")
```
Remove NAs if needed. Note that database is large so removing few NAs won't bias the analysis.
```{r}
feature_matrix=feature_matrix[complete.cases(feature_matrix),]
```

new basic dimensions after NAs
```{r}
n_obs  <- nrow(feature_matrix)
n_feat <- ncol(feature_matrix)

cat("Observations:", n_obs, "\n")
cat("Features:", n_feat, "\n")
```

# ----- 3. Feature_only data ----------------------------------
Most of the analysis will be on the matrix containing only numeric values.
We will exclude other idetification columns such as label, series_id, label_m (submodel), series, and series length.
We obtain the "feature_only" matrix.
```{r feature only}
feature_names <- lubridate::setdiff(names(feature_matrix), c("Max_logLik","series_id", "label", "label_m", "series", "T_length"))

feature_only <- feature_matrix[, colnames(feature_matrix) %in% (feature_names)]
head(feature_only,1)
# feature_only <- feature_matrix[, sapply(feature_matrix, is.numeric)]
```

In case there are any problems with the feature_only matrix, we will convert all columns to numerical
```{r all numeric}
feature_only <- feature_only %>% mutate_if(is.factor, as.character) %>% mutate_all(as.numeric)
```

For later use, we would need to keep the label of each series stored in "regime" vector.
```{r regime}
regime = feature_matrix$label
```

Basic dimensions
```{r Basic dimensions}
n_obs  <- nrow(feature_only)
n_feat <- ncol(feature_only)

cat("Observations:", n_obs, "\n")
cat("Features:", n_feat, "\n")
```
# ----- 4. Degeneracy ---------------------------------------
Check for constant or near-constant features.
```{r variance_summary}
variance_summary <- tibble(
  feature = names(feature_only),
  variance = apply(feature_only, 2, var, na.rm = TRUE),
  unique_vals = apply(feature_only, 2, function(z) length(unique(z)))
) %>% dplyr::arrange(variance)
```

These features will be removed immediately as they are obviously redundant and degenerate.

      Interpretation: 

      Zero variance → remove
      
      Two unique values → possible indicator, verify meaning
      
      High NA → consider imputation or exclusion
```{r degenrate features}
degenerate_features <- subset(variance_summary , variance == 0 | unique_vals <=2)

if (nrow(degenerate_features) > 0) {
  warning("Degenerate or quasi-degenerate features detected")
  print(degenerate_features)
}
```

Remove degenerate features
```{r remove degenrate features}
feature_only = feature_only[, !(colnames(feature_only) %in%  degenerate_features$feature)]
```

For later processing, replace -Inf with -1e5 ( only fix LogLik values)
```{r remove -Inf for logLik}
feature_only[grep("^logLik_", names(feature_only))] <-
  lapply(feature_only[grep("^logLik_", names(feature_only))], function(x) {
    x[x < -1e5] <- -1e5#Inf
    x[!is.finite(x)] <- -1e5
    x
  })
```

# ----- 5 . Supervised PCA (Sanity Check) ----------------

The aim is to check if generated dataset is really separable and can be well identified. i.e if the four models are well identified.
```{r PCA}
pca <- FactoMineR::PCA(
  feature_only,
  scale.unit = TRUE,
  graph = FALSE
)
```

Variance explained Scree plot

Interpretation : Few PCs explaining most variance → redundancy

We aim to have large number of PCs to explain more than 70-90% of the variance.

    Look for: 
    
    Sharp drop (elbow)
    
    Flat tail → noise
```{r scree plot}
factoextra::fviz_eig(pca, addlabels = TRUE)
```

How many components should you keep?

You should not keep components based on visualization alone.

Explained variance criterion (baseline)
      
      Rules of thumb:

      1 - Keep components explaining 70–90% of variance

      2 - Typically yields 10–30 components from 100 variables
```{r PC to keep}
which(cumsum(pca$eig[,2]) >= 90)[1]
```
How to use PCA with known groups

We have 4 known models, so PCA should be combined with a supervised method.

Visual inspection only (exploratory)
Plot individuals and should be clearly clustered into four distinct groups.
```{r ind plot pca}
factoextra::fviz_pca_ind(pca,
             habillage = regime,
             addEllipses = TRUE,
             ellipse.level = 0.95
)
```


Feature Contribution to first PCs
```{r var plot pca}
#loadings
# print(pca$var$coord)

factoextra::fviz_pca_var(pca, col.var = "contrib")

```

which variables define each PC
```{r var contrib pca}
head(pca$var$contrib, 10)
```

# ----- 6. k-means preview -----------------------
The elbow Method helps determine the optimal number of clusters.

There should be 4 clusters each for one model.

```{r elbow method kmeans}
factoextra::fviz_nbclust(scale(feature_only), kmeans, method = "wss")
```
Run k-means
```{r kmeans model}
k_means_result <- stats::kmeans(scale(feature_only), centers = 4, nstart = 20)
```

Plot kmeans. They should be 4 distinct clusters

```{r kmeans plot}
factoextra::fviz_cluster(k_means_result, 
             data = scale(feature_only),
             geom = "point", # Use points to represent observations
             ellipse.type = "convex", # Drawing a concentration ellipse around each cluster
             palette = "jco", # Color palette
             main = "K-Means Cluster Visualization (PCA-based)")

```
Clusters

Check whether features contain structure before labels

If clusters align with known regimes → strong signal

If not → classification may rely on subtle margins
```{r}
clusters = data.frame( features = regime, submodel = feature_matrix$label_m, cluster = k_means_result$cluster)
table(clusters)
```
# ----- 7. Univariate Description ---------------------------
Univariate summary statistics
    
    What to look for
    
    Heavy skewness → log / rank transforms
    
    Extreme kurtosis → tail-driven statistics (records, extremes)

```{r describe stat}
desc_stats <- psych::describe(feature_only)

print(desc_stats)
```

# ----- 8.  Correlation analysis ----------------------------

Pairwise correlation (Spearman recommended)
```{r cor mat}
cor_mat <- cor(feature_only, use = "pairwise.complete.obs", method = "spearman")

```


Visual inspection

We aim to have only clear (low correlation) zones.
```{r corrplot}
corrplot::corrplot(cor_mat,
         method = "color",
         type = "upper",
         tl.cex = 0.6,
         order = "hclust")
```

Identify highly correlated pairs

    Decision rule

    |ρ| > 0.95 → keep one representative
    Prefer:
    More interpretable
    Lower variance inflation
    Better downstream SHAP signal

```{r high corr}
high_corr <- which(abs(cor_mat) > 0.95 & abs(cor_mat) < 1, arr.ind = TRUE) 

high_corr_pairs <- tibble(
  feature_1 = rownames(cor_mat)[high_corr[,1]],
  feature_2 = colnames(cor_mat)[high_corr[,2]],
  rho = cor_mat[high_corr]
   ) %>% dplyr::filter(feature_1 != feature_2)

high_corr_pairs <- high_corr_pairs %>%
  rowwise() %>%
  dplyr::mutate(pair = paste(sort(c(feature_1, feature_2)), collapse = "_")) %>%
  ungroup() %>%
  dplyr::distinct(pair, .keep_all = TRUE) %>%
  dplyr::select(-pair)


print(high_corr_pairs)
```


# ----- 9. Outputs ------------------------------------------

Save output to R data

```{r save output EDA}
eda_output <- list(
  desc_stats = desc_stats,
  na_summary = na_summary,
  variance_summary = variance_summary,
  correlation_matrix = cor_mat,
  pca = pca,
  kmeans = k_means_result
)

saveRDS(eda_output, "C:/Users/User/Documents/Records/data/feature_EDA_summary.rds")
```

# ------ Group-wise summaries -----------------------

```{r group desc}
group_desc <- feature_only %>%
  dplyr::bind_cols(label = feature_matrix$label) %>%
  dplyr::group_by(label) %>%
  dplyr::summarise(
    across(
      where(is.numeric),
      list(
        mean = ~ mean(.x, na.rm = TRUE),
        #sd   = ~ sd(.x, na.rm = TRUE),
        med  = ~ median(.x, na.rm = TRUE)
        #IQR  = ~ IQR(.x, na.rm = TRUE)
      ),
      .names = "{.col}_{.fn}"
    ),
    .groups = "drop"
  ) %>%
  dplyr::select(-label) %>%
  t() %>%
  as.data.frame() %>%
  tibble::rownames_to_column("feature") %>%
  setNames(c("feature", "Classical", "DTRW", "LDM", "YNM")) %>%
  dplyr::mutate(across(-feature, as.numeric)) %>%
  rowwise() %>%
  dplyr::mutate(
    CV = abs(
      sd(c_across(Classical:YNM)) /
      mean(c_across(Classical:YNM))
    )
  ) %>%
    dplyr::mutate(
    range = abs(
      diff(range(c_across(Classical:YNM))) /
      mean(c_across(Classical:YNM))
    )
  ) %>%
  ungroup() %>% 
  arrange(CV) %>%
  dplyr::mutate(across(where(is.numeric), round, digits = 4))


print(head(group_desc, 10))
```

Variability is computed as the coefficient of variation.

    Interpretation
    
    CV < 0.05–0.10 → essentially invariant

    CV 0.10–0.30 → moderate differences

    CV > 0.30 → strong model dependence
```{r group desc invariant}
print(group_desc %>% dplyr::filter(CV < 0.1))
```


## recommended to remove
group_desc[which(group_desc$variability ==0 ), ]

group_desc[which(group_desc$variability <0.05 ), ]
# Interpretation
# Identical summaries across regimes → drop candidate
# Large IQR shifts → tail-sensitive feature

